# Abstract

Early detection is critical in melanoma diagnosis - treatment success strongly depends on catching the disease at an early stage. Dermoscopic imaging has become standard in clinical settings, but interpreting these images requires specialized training that many primary care providers don't have. My work tackles this problem by developing a deep learning system that combines accurate classification with explainability features that clinicians can actually use.

I implemented and compared four neural network architectures (ResNet-50, EfficientNet-B3, DenseNet-121, and Vision Transformer ViT-B/16) on the HAM10000 dataset, which contains 10,013 dermoscopic images across seven diagnostic categories. I trained each model for 20 epochs using the same preprocessing and training setup to make sure the comparison was fair. The system uses temperature scaling to calibrate probabilities and sets operating thresholds at 95% specificity, which reflects what's actually needed in clinical practice where false positives have real costs.

EfficientNet-B3 came out as the best architecture, achieving 89.22% accuracy overall and 95.34% AUC specifically for melanoma detection. At the clinical operating point, the model reached 80.72% sensitivity for melanoma while keeping 95% specificity, with inference taking only 10.06 milliseconds - fast enough for real-time use. Temperature scaling brought calibration errors below 3% across all models, meaning the probability estimates are actually reliable.

Beyond just classification, the system provides visual explanations using Grad-CAM (Gradient-weighted Class Activation Mapping) that highlight which parts of the image influenced the model's decision. I validated that these attention maps actually correlate with the ABCDE melanoma detection criteria that dermatologists use, so the explanations are medically meaningful rather than just highlighting random discriminative features. I built an interactive web interface with Gradio where healthcare providers can upload images, get calibrated probabilities, view the attention heatmaps, and read AI-generated explanations that put the predictions in clinical context.

The main contributions are: (1) systematic comparison of modern architectures under identical conditions, (2) combining temperature calibration with clinically-motivated operating thresholds, (3) explainable AI that aligns with actual dermatological criteria, and (4) a complete deployable web interface. Results show that deep learning can match dermatologist-level performance while providing the transparency clinicians need to trust it. I've documented everything - trained models, calibration parameters, deployment code - for reproducibility and potential use in other medical imaging tasks.
